{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# TP : Implémentation de l'algorithme de Viterbi pour le POS Tagging\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Dans ce TP, nous allons implémenter pas à pas l'algorithme de Viterbi pour l'étiquetage morphosyntaxique (POS Tagging). L'objectif est de comprendre comment trouver la séquence d'étiquettes grammaticales la plus probable pour une phrase.\n",
        "\n",
        "**Vu la difficulté de l'algorithme, les fonctions ont été pré-réalisées.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1"
      },
      "source": [
        "## Partie 1 : Préparation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex1-1"
      },
      "source": [
        "### Exercice 1.1 : Corpus d'entraînement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Soit le corpus dans la variable `training_data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_data"
      },
      "outputs": [],
      "source": [
        "# Corpus d'entraînement\n",
        "training_data = [\n",
        "    [(\"Le\", \"DET\"), (\"chat\", \"NOUN\"), (\"mange\", \"VERB\"), (\"la\", \"DET\"), (\"souris\", \"NOUN\")],\n",
        "    [(\"La\", \"DET\"), (\"souris\", \"NOUN\"), (\"mange\", \"VERB\"), (\"le\", \"DET\"), (\"fromage\", \"NOUN\")],\n",
        "    [(\"Le\", \"DET\"), (\"chien\", \"NOUN\"), (\"court\", \"VERB\"), (\"vite\", \"ADV\")],\n",
        "    [(\"Un\", \"DET\"), (\"oiseau\", \"NOUN\"), (\"chante\", \"VERB\"), (\"bien\", \"ADV\")],\n",
        "    [(\"La\", \"DET\"), (\"fille\", \"NOUN\"), (\"lit\", \"VERB\"), (\"un\", \"DET\"), (\"livre\", \"NOUN\")]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex1-1-q"
      },
      "source": [
        "**Questions**:\n",
        "1. Lister toutes les étiquettes différentes\n",
        "2. Compter le nombre total de phrases\n",
        "3. Identifier le mot le plus fréquent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2"
      },
      "source": [
        "## Partie 2 : Calcul des probabilités"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex2-1"
      },
      "source": [
        "### Exercice 2.1 : Calcul du vecteur des distributions initiales\n",
        "\n",
        "**Question** : \n",
        "1. Que mesure le vecteur des probabilités intiales $\\pi$ dans le contexte de l'algorithme de Viterbi? Comment peut-on le calculer?\n",
        "2. Compléter la fonction `compute_initial_probabilities` qui prend en entrée le training data et qui output un dictionaire qui contient pour chaque POS la probabilité d'une phrase de commencer par ce POS.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "initial_probs"
      },
      "outputs": [],
      "source": [
        "def compute_initial_probabilities(training_data):\n",
        "    \"\"\"\n",
        "    Calcule P(tag|START) - la probabilité qu'une phrase commence par une étiquette donnée.\n",
        "    \"\"\"\n",
        "    initial_counts = {}\n",
        "    total_sentences = len(training_data)\n",
        "    \n",
        "    for sentence in training_data:\n",
        "        if sentence:  # si la phrase n'est pas vide\n",
        "            first_tag = sentence[0][1]  # étiquette du premier mot\n",
        "            # À compléter : compter les occurrences de first_tag\n",
        "    \n",
        "    # Calcul des probabilités\n",
        "    initial_probs = {}\n",
        "    for tag, count in initial_counts.items():\n",
        "        # À compléter : calculer la probabilité\n",
        "        pass\n",
        "    \n",
        "    return initial_probs\n",
        "\n",
        "# Test\n",
        "initial_probs = compute_initial_probabilities(training_data)\n",
        "print(\"Probabilités initiales:\", initial_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex2-2"
      },
      "source": [
        "### Exercice 2.2 : Probabilités de transition\n",
        "\n",
        "**Question** : \n",
        "1. Rappelez comment est calculé la matrice des probabilités de transition.\n",
        "2. Complétez la fonction `compute_transition_probabilities` qui permet de retourner la matrice de transition au format:\n",
        "```\n",
        "    {\n",
        "        \"tag_current\": {\n",
        "            \"tag_next2\": proba,\n",
        "            \"tag_next1\": proba\n",
        "        }\n",
        "    }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "transition_probs"
      },
      "outputs": [],
      "source": [
        "def compute_transition_probabilities(training_data):\n",
        "    \"\"\"\n",
        "    Calcule P(tag2|tag1) - la probabilité de passer de tag1 à tag2.\n",
        "    \"\"\"\n",
        "    transition_counts = {}\n",
        "    tag_counts = {}\n",
        "    \n",
        "    for sentence in training_data:\n",
        "        for i in range(len(sentence) - 1):\n",
        "            current_tag = sentence[i][1]\n",
        "            next_tag = sentence[i+1][1]\n",
        "            \n",
        "            # À compléter : \n",
        "            # 1. Compter les transitions current_tag → next_tag\n",
        "            # 2. Compter combien de fois chaque tag apparaît comme current_tag\n",
        "    \n",
        "    # Calcul des probabilités\n",
        "    transition_probs = {}\n",
        "    for current_tag, next_tags in transition_counts.items():\n",
        "        transition_probs[current_tag] = {}\n",
        "        total_transitions = tag_counts[current_tag]\n",
        "        \n",
        "        for next_tag, count in next_tags.items():\n",
        "            # À compléter : calculer P(next_tag|current_tag)\n",
        "            pass\n",
        "    \n",
        "    return transition_probs\n",
        "\n",
        "# Test\n",
        "transition_probs = compute_transition_probabilities(training_data)\n",
        "print(\"Probabilités de transition:\", transition_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex2-3"
      },
      "source": [
        "### Exercice 2.3 : Probabilités d'émission\n",
        "\n",
        "**Question** : \n",
        "1. Rappelez comment est calculé la matrice des probabilités d'émission.\n",
        "2. Complétez la fonction `compute_emission_probabilities` qui calcule pour chaque mot la probabilité que celui-ci soit émis par son étiquette, avec le format:\n",
        "```\n",
        "{\n",
        "    \"tag1\": {\n",
        "        \"word1\": prob1,\n",
        "        \"word2\": prob2,\n",
        "        ...\n",
        "    },\n",
        "    \"tag2\": {\n",
        "        \"word1\": prob1,\n",
        "        \"word2\": prob2\n",
        "    }\n",
        "}\n",
        "```\n",
        "3. Retournez cette matrice sur le corpus d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emission_probs"
      },
      "outputs": [],
      "source": [
        "def compute_emission_probabilities(training_data):\n",
        "    \"\"\"\n",
        "    Calcule P(word|tag) - la probabilité qu'un mot soit émis par une étiquette.\n",
        "    \"\"\"\n",
        "    emission_counts = {}\n",
        "    tag_word_counts = {}\n",
        "    \n",
        "    for sentence in training_data:\n",
        "        for word, tag in sentence:\n",
        "            # À compléter :\n",
        "            # 1. Compter combien de fois un mot apparaît avec une étiquette\n",
        "            # 2. Compter combien de mots sont émis par chaque étiquette\n",
        "            pass\n",
        "    # Calcul des probabilités\n",
        "    emission_probs = {}\n",
        "    for tag, words in emission_counts.items():\n",
        "        emission_probs[tag] = {}\n",
        "        total_words = tag_word_counts[tag]\n",
        "        \n",
        "        for word, count in words.items():\n",
        "            # À compléter : calculer P(word|tag)\n",
        "            pass\n",
        "    \n",
        "    return emission_probs\n",
        "\n",
        "# Test\n",
        "emission_probs = compute_emission_probabilities(training_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part3"
      },
      "source": [
        "## Partie 3 : L'algorithme de Viterbi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex3-2"
      },
      "source": [
        "### Exercice 3.2 : Étape d'initialisation\n",
        "\n",
        "**Question**:\n",
        "1. Quelle est la phase d'initialisation de l'algorithme?\n",
        "2. Quelles sont les tables à maintenir pour réussir à calculer/reconstruire le trajet de l'algorithme?\n",
        "3. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viterbi_init"
      },
      "outputs": [],
      "source": [
        "def viterbi_initialize(first_word,\n",
        "                       initial_probs,\n",
        "                       emission_probs,\n",
        "                       all_tags):\n",
        "    \"\"\"\n",
        "    Initialise les tables Viterbi pour le premier mot de la phrase.\n",
        "    \"\"\"\n",
        "    viterbi_table = {}\n",
        "    backpointer = {}\n",
        "    \n",
        "    for tag in all_tags:\n",
        "        # À compléter :\n",
        "        # 1. Calculer la probabilité : P(tag|START) * P(first_word|tag)\n",
        "        # 2. Initialiser viterbi_table[tag] avec cette probabilité\n",
        "        # 3. backpointer[tag] = None (pas d'étiquette précédente)\n",
        "        pass\n",
        "    \n",
        "    return viterbi_table, backpointer\n",
        "\n",
        "# Test\n",
        "first_word = \"Le\"\n",
        "viterbi_table, backpointer = viterbi_initialize(first_word, initial_probs, emission_probs, unique_tags)\n",
        "print(\"Table Viterbi après initialisation:\", viterbi_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex3-3"
      },
      "source": [
        "### Exercice 3.3 : Étape de récursion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Questions**:\n",
        "\n",
        "1. Expliquez comment est calculé un pas de l'algorithme de viterbi.\n",
        "2. Complétez la fonction `viterbi_step` qui permet de passer d'une étape à une autre, en retournant pour chacun des tags possibles la probabilité et le chemin qui a permis de mener à cette valeur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viterbi_step"
      },
      "outputs": [],
      "source": [
        "def viterbi_step(word, prev_viterbi, transition_probs, emission_probs, all_tags):\n",
        "    \"\"\"\n",
        "    Calcule un pas de l'algorithme de Viterbi.\n",
        "    \"\"\"\n",
        "    current_viterbi = {}\n",
        "    current_backpointer = {}\n",
        "    \n",
        "    for current_tag in all_tags:\n",
        "        max_prob = -float('inf')\n",
        "        best_prev_tag = None\n",
        "        \n",
        "        for prev_tag in all_tags:\n",
        "            if prev_tag in prev_viterbi:\n",
        "                # À compléter :\n",
        "                # 1. Calculer la probabilité transition : prev_viterbi[prev_tag] * P(current_tag|prev_tag)\n",
        "                # 2. Garder la probabilité maximale et le best_prev_tag\n",
        "                pass\n",
        "        \n",
        "        # Ajouter la probabilité d'émission\n",
        "        if word in emission_probs[current_tag]:\n",
        "            # À compléter : multiplier par P(word|current_tag)\n",
        "            pass\n",
        "        \n",
        "        current_viterbi[current_tag] = max_prob\n",
        "        current_backpointer[current_tag] = best_prev_tag\n",
        "    \n",
        "    return current_viterbi, current_backpointer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex3-4"
      },
      "source": [
        "### Exercice 3.4 : Reconstruction du chemin\n",
        "\n",
        "**Question**\n",
        "1. Étant donné l'ensemble du chemin qui est stocké dans le dictionnaire `current_backpointer` et la dernière itération de l'algorithme, reconstruisez l'ensemble du chemin optimal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "backtrack"
      },
      "outputs": [],
      "source": [
        "def backtrack(backpointers, final_viterbi):\n",
        "    \"\"\"\n",
        "    Reconstruit la séquence d'étiquettes optimale à partir des backpointers.\n",
        "    \"\"\"\n",
        "    best_path = []\n",
        "    \n",
        "    # Trouver la meilleure étiquette finale\n",
        "    best_final_tag = None\n",
        "    max_prob = -float('inf')\n",
        "    \n",
        "    for tag, prob in final_viterbi.items():\n",
        "        # À compléter : trouver l'étiquette avec la probabilité maximale\n",
        "        pass\n",
        "    \n",
        "    # Reconstruction à rebours\n",
        "    current_tag = best_final_tag\n",
        "    for t in range(len(backpointers)-1, -1, -1):\n",
        "        # À compléter : ajouter l'étiquette courante et remonter au précédent\n",
        "        pass\n",
        "    \n",
        "    # À compléter : inverser la liste pour avoir l'ordre correct\n",
        "    return best_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part4"
      },
      "source": [
        "## Partie 4 : Assemblage complet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex4-1"
      },
      "source": [
        "### Exercice 4.1 : Fonction Viterbi complète"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question**:\n",
        "1. Étant donné l'ensemble des fonctions préparées précédemment, créez la fonction `viterbi` qui retourne pour une phrase donnée et pour une configuration d'HMM donnée la séquence de POS la plus probable.\n",
        "2. Appliquez la fonction à la phrase \"Le chat mange la souris\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viterbi_complete"
      },
      "outputs": [],
      "source": [
        "def viterbi(sentence,\n",
        "            initial_probs,\n",
        "            transition_probs,\n",
        "            emission_probs,\n",
        "            all_tags):\n",
        "    \"\"\"\n",
        "    Implémentation complète de l'algorithme de Viterbi.\n",
        "    \"\"\"\n",
        "    n = len(sentence)\n",
        "    if n == 0:\n",
        "        return []\n",
        "    \n",
        "    # Initialisation\n",
        "    viterbi_tables = [{}] * n\n",
        "    backpointers = [{}] * n\n",
        "    \n",
        "    # À compléter : \n",
        "    # 1. Appeler viterbi_initialize pour le premier mot\n",
        "    # 2. Pour chaque mot suivant, appeler viterbi_step\n",
        "    # 3. Reconstruire le chemin avec backtrack\n",
        "    \n",
        "    return best_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex4-2"
      },
      "source": [
        "### Exercice 4.2 : Évaluation\n",
        "\n",
        "**Question** :\n",
        "1. Rappelez la formule de l'accuracy\n",
        "2. Complétez la fonction `evaluate_tagger` qui permet de retourner l'accuracy étant donné une phrase, une ground truth, et la description d'un HMM. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluation"
      },
      "outputs": [],
      "source": [
        "def evaluate_tagger(test_sentences, true_tags, initial_probs, transition_probs, emission_probs, all_tags):\n",
        "    \"\"\"\n",
        "    Évalue la performance du tagger.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for i, sentence in enumerate(test_sentences):\n",
        "        predicted = viterbi(sentence, initial_probs, transition_probs, emission_probs, all_tags)\n",
        "        true = true_tags[i]\n",
        "        \n",
        "        print(f\"Phrase: {' '.join(sentence)}\")\n",
        "        print(f\"Vrai: {true}\")\n",
        "        print(f\"Prédit: {predicted}\")\n",
        "        \n",
        "        # À compléter : calculer le nombre d'étiquettes correctes\n",
        "        for j in range(len(sentence)):\n",
        "            total += 1\n",
        "            if predicted[j] == true[j]:\n",
        "                correct += 1\n",
        "        \n",
        "        accuracy = correct / total\n",
        "        print(f\"Accuracy: {accuracy:.2%}\")\n",
        "        print(\"-\" * 40)\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "# Données de test\n",
        "test_sentences = [\n",
        "    [\"Le\", \"chien\", \"mange\"],\n",
        "    [\"La\", \"fille\", \"lit\"]\n",
        "]\n",
        "true_tags = [\n",
        "    [\"DET\", \"NOUN\", \"VERB\"],\n",
        "    [\"DET\", \"NOUN\", \"VERB\"]\n",
        "]\n",
        "\n",
        "accuracy = evaluate_tagger(test_sentences, true_tags, initial_probs, transition_probs, emission_probs, unique_tags)\n",
        "print(f\"Précision finale: {accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part5"
      },
      "source": [
        "## Partie 5 : Gestion des mots inconnus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5-1"
      },
      "source": [
        "**Question** : \n",
        "1. Essayez d'utiliser votre tagger sur un corpus contenant un mot que vous n'avez pas encore vu.\n",
        "2. Quelle solution pouvez-vous proposer?\n",
        "2. Adaptez votre tagger pour qu'il puisse gérer les mots inconnus.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
